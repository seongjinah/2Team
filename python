from flask import Flask
from flask import render_template
from flask import request
from bs4 import BeautifulSoup
from numpy import dot
import numpy.linalg as npl
import requests
import time
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import math
import sys
from elasticsearch import Elasticsearch

word_d = {}

def compute_tf(s):
    bow = set()
    wordcount_d = {}

    tokenized = word_tokenize(s)
    for tok in tokenized:
        if tok not in wordcount_d.keys():
            wordcount_d[tok] = 0
        wordcount_d[tok] += 1
        bow.add(tok)

    tf_d = {}
    for word, count in wordcount_d.items():
        tf_d[word] = float(count)/len(bow)

    return tf_d

def compute_idf():
    Dval = len(content_list)
    bow = set()

    for cl in content_list:
        tokenized = word_tokenize(cl)
        for tok in tokenized:
            bow.add(tok)

    idf_d = {}
    for t in bow:
        cnt = 0
        for s in content_list:
            if t in word_tokenize(s):
                cnt += 1
        idf_d[t] = math.log(Dval/(cnt+1))

    return idf_d

def make_word_d():
    for i in range(len(content_list)):
        tokenized = word_tokenize(content_list[i])
        for word in tokenized:
            if word not in word_d.keys():
                word_d[word] = 0
            word_d[word] += 1

def make_vector(i):
    v = []
    s = content_list[i]
    tokenized = word_tokenize(s)
    for w in word_d.keys():
        val = 0
        for t in tokenized:
            if t==w:
                val += 1
        v.append(val)
    return v

#######################################################################################
#main
#filename = input("파일이름 입력:")

nltk.download("stopwords")

swlist = []
for sw in stopwords.words("english"):
    swlist.append(sw)

web_list = []
with open('textfile.txt', 'r') as f:
    while 1:
        line = f.readline().split()
        if not line: break
        web_list.extend(line)

multi = False
if(len(web_list) != len(set(web_list))):
    multi = True
lines = list(set(web_list))

time_list = []
num_list = []
content_list = []
'''
for j in range(len(lines)):
    start = time.time()

    address = web_list[j]

    res = requests.get(address)
    soup = BeautifulSoup(res.content, 'html.parser')

    p = soup.find_all('p')
    h1 = soup.find_all('h1')
    h2 = soup.find_all('h2')
    h3 = soup.find_all('h3')
    h4 = soup.find_all('h4')
    h5 = soup.find_all('h5')
    h6 = soup.find_all('h6')
    li = soup.find_all('li')

    s = ''

    for i in p:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h1:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h2:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h3:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h4:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h5:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h6:
        s += i.text.replace('\n', '').strip() + ' '

    for i in li:
        s += i.text.replace('\n', '').strip() + ' '
    if s == '':
     value = value + 1

    #특수문자 제거
    s = re.sub('[\[\]\/!@#$%^&*().,:]', ' ', s)

    # 단어수 계산
    num_list.append(len(s.split()))

    #시간 계산
    time_list.append(time.time()-start)

    #s에서 stopwords 제외 -> s_list
    s_list = []
    tokenized = word_tokenize(s)
    for tok in tokenized:
        if tok not in swlist:
            s_list.append(tok)

    #깔끔한 s(s_list)를 content_list에 저장
    s = ' '.join(s_list)
    content_list.append(s)
'''

content_list = []
for i in range(len(lines)):
    filename = str(i) + 'crawling.txt'
    with open(filename, 'r', encoding='utf8') as f:
        content_list.append(f.readline())

#tf-idf 계산
idf_d = compute_idf()
tfidf_list = []
for cl in content_list:
    tf_d = compute_tf(cl)

    sort_tf_d = sorted(tf_d.items(), reverse=True, key=lambda item: item[1])
    top_10_set = list(sort_tf_d[:10])
    top_10 = []
    for t10 in top_10_set:
        top_10.append(t10[0])
    tfidf_list.append(top_10)

#Cosine Similarity
make_word_d()
vector = []
cosine_list = []

for i in range(len(content_list)):
    vector.append(make_vector(i))

for i in range(len(content_list)):
    cosine = {}
    for j in range(len(content_list)):
        if i == j: continue
        dotpro = dot(vector[i], vector[j])
        cossimil = dotpro/(npl.norm(vector[i]) * npl.norm(vector[j]))
        cosine[j] = cossimil
    cosine_d = sorted(cosine.items(), reverse=True, key=lambda item: item[1])
    top_3_set = list(cosine_d[:3])
    top_3 = []
    for t3 in top_3_set:
        top_3.append(t3[0])
    cosine_list.append(top_3)
