import math
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import requests
from bs4 import BeautifulSoup
import re

word_d = {}
content_list = []

web_list = ['https://cassandra.apache.org/', 'http://empire-db.apache.org/', 'http://hive.apache.org/',
            'http://hbase.apache.org/', 'https://camel.apache.org/', 'http://mesos.apache.org/',
            'https://ignite.apache.org/']

def compute_tf(s):
    bow = set()
    wordcount_d = {}

    tokenized = word_tokenize(s)
    for tok in tokenized:
        if tok not in wordcount_d.keys():
            wordcount_d[tok] = 0
        wordcount_d[tok] += 1
        bow.add(tok)

    tf_d = {}
    for word, count in wordcount_d.items():
        tf_d[word] = float(count)/len(bow)

    return tf_d

def compute_idf():
    Dval = len(content_list)
    bow = set()

    for cl in content_list:
        tokenized = word_tokenize(cl)
        for tok in tokenized:
            bow.add(tok)

    idf_d = {}
    for t in bow:
        cnt = 0
        for s in content_list:
            if t in word_tokenize(s):
                cnt += 1
        idf_d[t] = math.log(Dval/(cnt+1))

    return idf_d


#main
nltk.download("stopwords")

swlist = []
for sw in stopwords.words("english"):
    swlist.append(sw)

for j in range(len(web_list)):
    res = requests.get(web_list[j])
    soup = BeautifulSoup(res.content, 'html.parser')

    p = soup.find_all('p')
    h1 = soup.find_all('h1')
    h2 = soup.find_all('h2')
    h3 = soup.find_all('h3')
    h4 = soup.find_all('h4')
    h5 = soup.find_all('h5')
    h6 = soup.find_all('h6')
    li = soup.find_all('li')

    s = ''

    for i in p:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h1:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h2:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h3:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h4:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h5:
        s += i.text.replace('\n', '').strip() + ' '

    for i in h6:
        s += i.text.replace('\n', '').strip() + ' '

    for i in li:
        s += i.text.replace('\n', '').strip() + ' '

    s_list = []
    tokenized = word_tokenize(s)
    for tok in tokenized:
        if tok not in swlist:
            s_list.append(tok)

    s = ' '.join(s_list)
    s = re.sub('[\[\]\/!@#$%^&*().,:]', '', s)
    content_list.append(s)

    filename = str(j) + '_web_crawling.txt'
    with open(filename, 'w', encoding='utf8') as f:
        f.write(s)

idf_d = compute_idf()
for cl in content_list:
    tf_d = compute_tf(cl)

    sort_tf_d = sorted(tf_d.items(), reverse=True, key=lambda item: item[1])
    top_10=list(sort_tf_d[:10])

    print(">>>tf-idf")
    for i in range(len(top_10)):
        print(i, top_10[i])
    print("")
